{"pages":[{"title":"About & Contact","text":"Hey, My name is Yuri Khomyakov.I am a senior software developer / tech lead and an aspiring software architect. I have been coding for the past 6 years. I like reading books about software architecture, design patterns, algorithms, programming paradigms, implementing what I have learned and progress as a software architect. I like building complex applications, cracking tough challenges and creating presentations and talks for my coworkers. I am open to open source collaborations, giving talks and talking over a cup of coffee. Email: yurik1776@gmail.comGithub: https://github.com/yuraxdrumzLinkedIn: https://www.linkedin.com/in/yuri-khomyakov-383278125/","link":"/about/index.html"}],"posts":[{"title":"Context Maps in Domain Driven Design","text":"Ideally, it would be great to have a single place that incorporates all of our models, but in reality, our systems fragment to multiple models and we need to understand how to approach building them in way that allows future changes quickly.Strategic Domain Driven Design is a high level approach to distributed software architecture and is an essential part of DDD. One of its features is context maps, which allows grasping the different relationships between bounded contexts (a boundary within which the ubiquitous language is consistent) and gives the teams a better understanding on how they affect each other. In this short post I will introduce you to the basics of context maps. How to approach Strategic DDDWhen working on a project, we can have teams that are large by count or located abroad, we can also have external systems and legacy systems to communicate with. The idea is to have some sort of wiki that teams can refer to, that is neither overly complex nor missing information. Understanding the relationships early on can help diagnose problems, as wrong relationships between components can quickly turn into a big ball of mud. Types of context mapsStarting from the top, each context map type has greater level of communication between teams at the expense of control over the domain: Shared Kernel - This is a shared domain model between two teams. Each team has an agreed upon subset of the domain along with its model. Partnership - The teams have a mutual dependency on each other for delivery. Customer-Supplier - The teams define interfaces to adhere and one team acts as a downstream (customer) to another team, which is the upstream (supplier). The upstream team can make changes without fear of breaking something downstream. The domains can evolve independently as long as the upstream context fulfills its interfaces. Open Host Service - A bounded context that offers a defined set of functionalities exposed to other systems. Any downstream system can implement their own integration. Published Language - Similar to Open Host Service, however it models a domain as a common language between bounded contexts Conformist - The downstream team conforms to the model of the upstream and there is no translation of models between them. If the upstream is a mess and the dev team behind it does not want / cant change it, the mess is propagated downstream. Anticorruption Layer (ACL) - A layer that isolates/abstracts the downstream’s models from another system’s models by translation. Separate Ways - There is no connection between the bounded contexts. The teams can find their own solutions in ther domains. ExampleOnce we identify what are our domains and boundaries we can start drawing context maps between them. In this example we can see the U for upstream and D for downstream. The online banking services acts as a supporting or a generic subdomain to our core PFM Banking domain. We use ACL when we receive a response and model it appropriately internally. We have a partnership relationship with the Expense Tracking Domain, which means we need to deliver together as we may have a mutual dependency. Last, we have a conformist relationship with the Web User Profiling. Maybe, the team cant change or wont change their implementation of a model, so we need to do it on our side, on the core domain. SummaryWe saw why do we need Strategic Domain Driven Design and how we can leverage a part of it called context maps to help us build better systems. We saw what kinds of context maps exist, what each one does and at last, we saw a small example with a few context maps.","link":"/2019/07/10/Context-Maps-In-Domain-Driven-Design/"},{"title":"Implementing Clean Architecture","text":"Last year, I had the opportunity to design a new project at work and since I had just finished reading Robert Martin’s Clean Architecture I thought to myself, why not implement it on that project ? One of the reasons, except my usual I have to implement this cool thing right away! was that working on legacy systems in the company was accompanied with the good ol’ big ball of mud code. The purpose of this post is to show you how can one implement Clean Architecture in practice and still understand it years from now, whether you work alone or in a team. Everything shown will be written in Typescript on Node.js using Object Oriented programming paradigm. All of the code will be available here DisclaimerSome of the things that I am going to write and show are my personal experiences and opinions, you may have read Robert Martin’s Clean Architecture and thought, interpreted or implemented otherwise. All the architectures have the same goals in the end. Implementing this in production has taught me a lot about how to build better software and expanded my toolbox as I hope it expands yours. Core IdeaThe idea behind Clean Architecture is that we have layers. Each layer is encapsulated by a higher level layer and the only way to communicate between the layers is with The Dependency Rule. The Dependency Rule states that source code dependencies can only point inwards, meaning each layer can be dependant on the layer beneath it, but never the other way around. EntitiesThe core of this architecture are your entities, which represent your classes/types/interfaces/basic methods. Use-CasesA layer above the entities layer is your use-cases. Use-cases are your application specific business rules, for example, if we are talking about a shopping cart, then addToCart will be a use case, because it needs to recieve a type product and, for examples sake, check warehouse for availability and then insert new data to a DB and return response. Do not couple your use-cases to some input or output, instead pass a contract (interface) of some type in the constructor and pass the implementation itself at higher layers. Repository PatternFor database interactions it is recommended to use the Repository Pattern which encapsulates all your database interactions through an abstraction layer. The repository pattern does give you a bit freedom to replace databases with ease, but this rule only applies when your interactions are basic CRUD operations! If you have many to many relationships which require a graph database, switching to mongodb at the repository layer will not help you much as it is not built for that purpose, so take interactions into consideration at design level! Interface AdaptersAfter the use-cases layer we have the Interface Adapters layer. Here, you convert your data from the form most convenient for entities and use cases, into the form most convenient for whatever persistence framework is being used, like the database, web or whatever you like. I like to call it, the implementations layer. Frameworks and DriversThe last layer is the Frameworks and Drivers. Here you call all of your dependencies that abide the contracts you defined in your use-cases. That way you can replace dependencies without the use-cases knowing anything about it, according to the L in S.O.L.I.D, which is called the Liskov substitution principle. Liskov Substitution PrincipleLiskov’s substitution principle states that if a system is using a type T which is an implementation of type S and we switch the implementation to type Z which is also of type S , the behaviour of the program should not change. A small diagram to illustrate our layers, notice the arrows only pointing inward! Taken from:https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html Clean Architecture In Practicelets build something overused, like a shopping cart. We will first decide what are our use cases and from that we would be able to conclude an initial data model - our entities. Later on, we will create Interface Adapters(implementations) and at the final layer we will simply glue all of our dependencies and implementations and see how clean architecture could benefit us in future projects. Last but not least, we will show how easy it is to switch implementations from a web server to a command line interface. What are our use-CasesBecause we chose a shopping cart, our use-cases will be pretty straight forward - addToCart and removeFromCart. Lets say addToCart needs to check our warehouse which is an external service, afterwards it will need to insert to our DB. removeFromCart will update warehouse and afterwards delete from our DB. After deciding on our business rules (use-cases), we can create an inital data model. Defining the typessrc\\entities\\types\\AddedToCart.ts 12type AddedToCart = booleanexport default AddedToCart src\\entities\\types\\RemovedFromCart.ts 12type RemovedFromCart = booleanexport default RemovedFromCart src\\entities\\types\\Product.ts 12type Product = {}export default Product src\\entities\\types\\ItemInWareHouse.ts 12type ItemInWareHouse = booleanexport default ItemInWareHouse Defining the contracts (interfaces)src\\entities\\interfaces\\addToCart.ts 12345import AddedToCart from '../types/AddedToCart'import Product from '../types/Product'export default interface IAddToCart { add(item: Product): Promise&lt;AddedToCart&gt;} src\\entities\\interfaces\\removeFromCart.ts 12345import RemovedFromCart from '../types/RemovedFromCart'import Product from '../types/Product'export default interface IRemoveFromCart { remove(item: Product): Promise&lt;RemovedFromCart&gt;} src\\entities\\interfaces\\cartRepository.ts 12345678import AddedToCart from '../types/AddedToCart'import RemovedFromCart from '../types/RemovedFromCart'import Product from '../types/Product'export default interface ICartRepository { add(item: Product): Promise&lt;AddedToCart&gt; remove(item: Product): Promise&lt;RemovedFromCart&gt;} src\\entities\\interfaces\\warehouse.ts 1234567import Product from '../types/Product'import ItemInWareHouse from '../types/ItemInWareHouse'export default interface IWarehouse { checkItemInWarehouse(item: Product): Promise&lt;ItemInWareHouse&gt; returnItemToWarehouse(item: Product): Promise&lt;ItemInWareHouse&gt;} Defining the use-casesNote how we expect to receive implementations of the warehouse and cartRepository interfaces in the code snippet below. The implementations themselves can be anything as long as they implement our interfaces.The implementations will be passed in the constructor as well as other 3rd party dependencies/modules. If we used/imported the dependencies/implementations directly, we would not adhere to the Clean Architecture's inwards dependency (arrows we saw in the diagram above) as we would create a dependency of a higher level module to a lower level detail, also called dependency inversion. src\\use-cases\\addToCart.ts 1234567891011121314151617181920212223242526import AddedToCart from '../entities/types/AddedToCart'import Product from '../entities/types/Product'import IWarehouse from '../entities/interfaces/warehouse'import ICartRepository from '../entities/interfaces/cartRepository'import IAddToCart from '../entities/interfaces/addToCart'abstract class AddToCart implements IAddToCart { protected cartRepository: ICartRepository protected warehouseService: IWarehouse constructor(cartRepository: ICartRepository, warehouseService: IWarehouse){ this.cartRepository = cartRepository this.warehouseService = warehouseService } async add(item: Product): Promise&lt;AddedToCart&gt; { const isItemInWarehouse = await this.warehouseService.checkItemInWarehouse(item) if(!isItemInWarehouse) return false const isSaved = await this.cartRepository.add(item) if(!isSaved) return false return true }}export default AddToCart src\\use-cases\\removeFromCart.ts 123456789101112131415161718192021222324252627import RemovedFromCart from '../entities/types/RemovedFromCart'import Product from '../entities/types/Product'import IRemoveFromCart from '../entities/interfaces/removeFromCart'import IWarehouse from '../entities/interfaces/warehouse'import ICartRepository from '../entities/interfaces/cartRepository'abstract class RemoveFromCart implements IRemoveFromCart { protected cartRepository: ICartRepository protected warehouseService: IWarehouse constructor(cartRepository, warehouseService){ this.cartRepository = cartRepository this.warehouseService = warehouseService } async remove(item: Product): Promise&lt;RemovedFromCart&gt; { const isItemReturned = await this.warehouseService.returnItemToWarehouse(item) if(!isItemReturned) return false const isItemDeleted = await this.cartRepository.remove(item) if(!isItemDeleted) return false return true }}export default RemoveFromCart Defining the implementationsNow lets first create implementations of add / remove / cartRepository and warehouse for a web server. src\\implementations\\addToCart\\web.ts 12345678910111213import AddToCart from '../../use-cases/addToCart'class ConcreteAddToCart extends AddToCart { async receiveProductFromWeb(request, response){ if(request &amp;&amp; request.body &amp;&amp; request.body[\"item\"]){ const isAdded = await this.add(request.body[\"item\"]) response.json(isAdded) } else { throw new Error(\"body is missing required field item\") } }}export default ConcreteAddToCart src\\implementations\\addToCart\\web.ts 1234567891011121314import RemoveFromCart from '../../use-cases/removeFromCart'class ConcreteRemoveFromCart extends RemoveFromCart { async removeProductFromWeb(request, response){ if(request &amp;&amp; request.body &amp;&amp; request.body[\"item\"]){ const isRemoved = await this.remove(request.body[\"item\"]) response.json(isRemoved) } else { throw new Error(\"body is missing required field item\") } }}export default ConcreteRemoveFromCart I added a console implementation of cartRepository, which simply logs to stdout. src\\implementations\\cartRepository\\console.ts 1234567891011121314151617import ICartRepository from '../../entities/interfaces/cartRepository'import Product from '../../entities/types/Product'import AddedToCart from '../../entities/types/AddedToCart'import RemovedFromCart from '../../entities/types/RemovedFromCart'class ConcreteCartRepository implements ICartRepository { async add(item: Product): Promise&lt;AddedToCart&gt; { console.log('adding item to database') return true } async remove(item: Product): Promise&lt;RemovedFromCart&gt; { console.log('removing item from database') return true }}export default ConcreteCartRepository I added a console implementation of warehouse, which simply logs to stdout. src\\implementations\\warehouse\\console.ts 1234567891011121314151617import IWarehouse from '../../entities/interfaces/warehouse'import Product from '../../entities/types/Product'import AddedToCart from '../../entities/types/AddedToCart'import RemovedFromCart from '../../entities/types/RemovedFromCart'class ConcreteWarehouse implements IWarehouse { async checkItemInWarehouse(item: Product): Promise&lt;AddedToCart&gt; { console.log('adding item to warehouse') return true } async returnItemToWarehouse(item: Product): Promise&lt;RemovedFromCart&gt; { console.log('returning item to warehouse') return true }}export default ConcreteWarehouse We chose a web implementation for addToCart and removeFromCart and a console implementation for warehouse and cartRepository. We wrapped each use-case with a handler which will be part of a web server, in other words, we prepared the data in this layer for the next layer to use, which is the frameworks and drivers layer. Defining the frameworks and driversNow, the last glue layer looks like this:We could make it prettier, but I will leave that to you, after we learn this cool new architecture! src\\frameworks-drivers\\web.ts 1234567891011121314151617181920212223242526272829303132333435import express from 'express'import bodyParser from 'body-parser'import CartRepositoryImpl from '../implementations/cartRepository/console'import WarehouseImpl from '../implementations/warehouse/console'import AddToCartWebImpl from '../implementations/addToCart/web'import RemoveFromCartWebImpl from '../implementations/removeFromCart/web'const app = express()const cartRepo = new CartRepositoryImpl()const warehouse = new WarehouseImpl()const addToCartInstance = new AddToCartWebImpl(cartRepo, warehouse)const removeFromCartInstance = new RemoveFromCartWebImpl(cartRepo, warehouse)app.use(bodyParser.json())app.post('/item', async (req,res,next)=&gt;{ try{ await addToCartInstance.receiveProductFromWeb(req, res) }catch(e){ next(e) }})app.delete('/item', async (req,res,next)=&gt;{ try{ await removeFromCartInstance.removeProductFromWeb(req, res) }catch(e){ next(e) }})app.listen(process.env.PORT, ()=&gt;{ console.log(`listening on port ${process.env.PORT}`)}) We initiated all dependencies, created all instances and passed everything along, if all interfaces are adhered, the code will compile and we can run web.ts. Try and run this example and send a POST to /item and a DELETE to /item, you will see our warehouse and database console implementations writing to stdout like we planned to. Creating another implementationNow, lets create a command line interface implementation for addToCart and removeFromCart. We will leave the warehouse and cartRepository as is but you can play with them as you wish. lets create a new cli.ts under implementations\\addToCartsrc\\implementations\\addToCart\\cli.ts 12345678910111213import AddToCart from '../../use-cases/addToCart'class ConcreteAddToCart extends AddToCart { async receiveProductFromCli(product){ if(product){ const isAdded = await this.add(product) console.log(`isAdded: ${isAdded}`) } else { console.error(\"commandLineOptions require product to be passed\") } }}export default ConcreteAddToCart src\\implementations\\removeFromCart\\cli.ts 12345678910111213import RemoveFromCart from '../../use-cases/removeFromCart'class ConcreteRemoveFromCart extends RemoveFromCart { async removeProductFromCli(product){ if(product){ const isRemoved = await this.remove(product) console.log(`isRemoved: ${isRemoved}`) } else { console.error(\"commandLineOptions require product to be passed\") } }}export default ConcreteRemoveFromCart Now, all we have left to do is create a new cli.ts under frameworks-drivers and call the corresponding cli implementations. src\\frameworks-drivers\\cli.ts 123456789101112131415161718192021222324252627282930313233343536import program from 'commander'import CartRepositoryImpl from '../implementations/cartRepository/console'import WarehouseImpl from '../implementations/warehouse/console'import AddToCartCliImpl from '../implementations/addToCart/cli'import RemoveFromCartCliImpl from '../implementations/removeFromCart/cli'program .option('-p, --product &lt;name&gt;', 'product name') .option('-a, --add', 'action') .option('-r, --remove', 'action')program.parse(process.argv);/** * to run this from typescript first run npm run dist and then node dist\\index.js -p \"test product name\" -a or node dist\\index.js -p \"test product name\" -r */(async ()=&gt;{ const cartRepo = new CartRepositoryImpl() const warehouse = new WarehouseImpl() const addToCartInstance = new AddToCartCliImpl(cartRepo, warehouse) const removeFromCartInstance = new RemoveFromCartCliImpl(cartRepo, warehouse) if(!program.product){ throw new Error(\"-p is required\") } if(program.add){ await addToCartInstance.receiveProductFromCli(program.product) } else if (program.remove){ await removeFromCartInstance.removeProductFromCli(program.product) } else { throw new Error(\"-a or -r are required\") }})() SummaryWe first looked at what Clean Architecture is, as defined by Robert Martin, later, we saw what each layer does and provided detailed examples. In the end, we created a new implementation without touching the core business rules (addToCart, removeFromCart).We saw how this architecture encourages seperation of concerns out of the box, gives a guideline on how to structure your code, defines interactions between layers and allows rapid changes regardless of the size of your codebase.","link":"/2019/06/11/Implementing-Clean-Architecture/"},{"title":"Functional Programming in 10 Minutes","text":"When I first saw the ideas of functional programming, I found them very strange due to the fact that, I, like most people, got used to structural and object oriented programming paradigms. The structural takes away the goto definitions from our code and replaces them with if/else/do/while, which forces the code to execute in an order. The object oriented, encapsulates local variables and methods long after a function returns (what eventually became a constructor) and through the use of function pointers introduced polymorphism. In this post, I will introduce you to functional programming. Functional Programming in a NutshellIt is the direct result of Alonso Church, who invented Lambda Calculus in 1936. FP treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data. Before I jump to the characteristics, I want to point out that the terms highlighted will be explained later on. All code examples will be written in Javascript. The Characteristics of Functional Programming Immutability - Once a variable is created, it cannot be mutated. Declerative - Describes what the program must accomplish in terms of the problem domain, rather than describe how to accomplish it as a sequence of the programming language primitives. Higher-order functions and functions as first class citizens - Functions can take other functions as arguments or return them as results. With the help of closures, functions allow currying and partial application. Pure functions - functions that have no side effects. Recursion - For loops inherently mutate state, remember incrementing i ? As FP does not allow mutating state everything is done with recursion. Do not worry about stackoverflow errors, as FP has optimized recursions called tail call recursions. We saw some characteristics, now, lets see what are closures, currying, partial application, side effects and tail call recursions. ClosuresThink of a function’s variables as a bag. When the function is returned (removed from the stack and its frame is destroyed), its bag remains. We previously said that functions can return other functions and closures come into play when an outer function is destroyed and the inner function wants to access previously passed arguments (the outer functions bag). 123456789101112function firstName(firstName){ return function lastName(lastName){ return firstName + lastName }}const firstNameWill = firstName(\"Will\")const lastNameSmith = firstNameWill(\"Smith\")const lastNameIAM = firstNameWill(\".I.AM\")/* prints Will Smith */console.log(lastNameSmith) /* prints Will.I.AM */console.log(lastNameIAM) You see how we stored &quot;Will&quot; in the “bag” of the firstName function. Even after it returned, we still had access to its variables. It allowed us to reuse the name Will and call it with different last names. CurryingIn mathematics and computer science, currying is the technique of translating the evaluation of a function that takes multiple arguments into evaluating a sequence of functions, each with a single argument. Lets see an example without currying 12345function entireName(firstName, middleName, lastName){ return firstName + middleName + lastName}console.log(entireName(\"Will\"))// prints \"Will undefined undefined\" With function currying, if we pass 1 argument, we will recieve a function back and only when all arguments are passed, our function will execute. 123456789101112131415161718192021222324252627function entireName(firstName, middleName, lastName){ return firstName + middleName + lastName}/* the above in its curried form lookes like this:*/function entireName(firstName){ return function middleName(middleName){ return function lastName(lastName){ return firstName + middleName + lastName } }}const withNameWill = entireName(\"Will\")const withMiddleNameI = withNameWill(\"I\")const withLastNameAM = withMiddleNameI(\"AM\")/*Note that purely functional languages curry by default, like `Haskell`, others have different libraries for automatic currying of functionsfor example with ramda.js we could wrap our entireName function in a curry function*/const { curry } = require(\"ramda\")function entireName(firstName, middleName, lastName){ return firstName + middleName + lastName}/* Below function is automatically curried like the example above and it is a lot more readable */const curriedEntireName = curry(entireName) Partial ApplicationPartial application works similar to currying. The difference between them is that currying splits a function to functions that receive one argument, also called unary, while partial application allows passing multiple arguments. Lets see an example 1234567891011121314function entireName(firstName, middleName, lastName){ return firstName + middleName + lastName}/* Look how we passed \"Will\" and \"I\" together. With currying you have to pass a single argument at a time until all arguments are passed. With partial Application you can pass any number of arguments and it will either execute if all arguments were given, or return a function that expects the original number of arguments minus the ones passed.*/const { partial } = require(\"ramda\")const partialWithTwoArgs = partial(entireName, [\"Will\", \"I\"])const fullName = partial(partialWithTwoArgs, [\"AM\"])console.log(fullName)/* prints \"Will I AM\"*/ Side EffectsIn programming, a side effect is when a procedure changes a variable from outside its scope.For example: 123function updateVariable(){ dbConnection.set(\"side_effect\", true)} We changed some state outside of our function - this is called a side effect.Functional programming is against side effects, but without side effects we would not be able to write software. Instead we try to limit the amount of side effects, allowing only a portion of our code to carefully do them. RecursionRecursion in computer science is a method of solving a problem where the solution depends on solutions to smaller instances of the same problem (as opposed to iteration). Basically, a function calls itself multiple times until some condition is met.For example, lets see a fibonacci recursion, do not mind the big O notation: 1234function fibonacci(num) { if (num &lt;= 1) return 1; return fibonacci(num - 1) + fibonacci(num - 2);} This function will call itself repeatedly until the condition above is met and until it does that, the function will create multiple frames of itself, possibly leading to stackoverflow.To address the recursion problem, tail-call optimization is used. Tail-call optimization is where you are able to avoid allocating a new stack frame for a function because the calling function will simply return the value that it gets from the called function. Usually, the compiler takes care of tail call optimization in functional languages, I will not elaborate on when tail call optimization does not occur, but if you get into FP in more detail, make sure to read about it. Bonus: PipesLets see how we put to practice all the above. In functional languages we usually have a pipe operator that allows an argument to pass through a series of functions. 12345678910111213141516/* Math: f(g(h(x, y)))*//* Equivalent JS*/const operate = (x, y) =&gt; square(addOne(multiply(x, y)))/* Same as above with pipe*/const operate = pipe( square, addOne, multiply)(x, y) If you are familiar with UNIX style programming, FP looks similar. 1ls -l | uniq | wc -l If you did the above in a terminal before, you have used functional programming! Each of the functions above does one thing without mutating side effects and with the use of the pipe(|) operator we are able to combine our functions to achieve our end goal. What functional programming helps achieveAll race conditions, deadlock conditions, and concurrent update problems are due to mutable variables. Instead of wiring a solution to these problems, we avoid them altogether ahead of time using functional programming! SummaryWe saw what is the definition of functional programming. Later, we saw the basic characteristics of it, like currying, partial application, immutability, higher order functions and recursion. Afterwards, we looked at a few examples and learned how to leverage FP to our advantage without mutating state. In the end, we saw how pipes in functional programming help us achieve readability and expressiveness.","link":"/2019/06/22/Functional-Programming-in-10-minutes/"},{"title":"Implementing Ports and Adapters","text":"There are different architectures that allow you to keep focus on your business domain and allow for fast paced development and changes. Examples would be: Clean Architecture, Onion Architecture and Ports and Adapters (also called hexagonal).In my previous post, I talked about Clean Architecture and how it helps get your code more modular and developer friendly after a somewhat short learning curve. After joining a new team, I noticed Clean Architecture did not really settle and the team found it to be somewhat over abstracted, so I decided to play around with a variation of Ports and Adapters.In this post I will show what is Ports and Adapters and how I implemented it in Golang. Github repo is available here. DisclaimerSome of the things that I am going to write and show are my personal experiences and opinions. Core IdeaPorts and Adapters architecture divides a system into several loosely-coupled interchangeable components. The components communicate with each other through abstracted API’s with the use of interfaces and their implementations.This approach is an alternative to the traditional layered architecture, where components are divided into layers. There are no layers in Ports and Adapters, only ports &lt;- adapters, meaning there are no restrictions on how to structure the applications, only that an adapter relies on a port and the business logic port relies on other ports Business LogicAll of your business specific use cases.Example: upon adding a user to a board, send an email, save the user in the database and grant the user permissions to view the board from an auth service. PortsThe interfaces to all of the components in your system. There are two kinds of ports: driven and driver. Driven PortsInterfaces that your application business logic uses for its needs. Driver PortsInterfaces for the outside world, a.k.a API AdaptersImplementations of our ports. They can be either driven or driver, depending on the port we use Driven AdapterExample: Service-To-Service adapter, for when we need to request some data from another service in our business logic use case Driver AdapterExample: GUI adapter, for when we need to convert events triggered by a GUI app to events defined by other ports PrerequisitesBecause Ports and Adapters does not define a specific folder structure, I will not focus on it, but you can take a reference from the structure in the github repository.One thing to note is I replaced the driven ports with the name out and the driver ports with the name in, as it confused developers that are new to Ports and AdaptersI like to name my packages as package/ports, and the adapters, like package/console.go, package/web.go, meaning there are two adapters, one is console and one is web for a specific port, that way it is easier to know what is implemented where.Regarding the structs, I prefer putting them in each package as a sub package, in order to avoid cyclic import dependencies. package/ports/... Ports and Adapters In Practicelets build a shopping cart. Our cart will have two use cases, addToCart and removeFromCart. Let’s say that we need a warehouse service and a database to save the changes. We can create one port for all the use cases regarding the cart, or we can do a port per use case in the cart. It is up to you and the level of abstraction you seek.I chose to put all the cart use cases under one port. Ports In PracticeDriven portsOur warehouse package ports will look like this:warehouse/ports.go 123456package warehousetype Port interface { CheckIfAvailable(itemID string) (bool, error) RemoveItemFromWarehouse(itemID string) (bool, error)} Our cart repository package ports will look like this:cartRepository/ports.go 12345678910package cartrepositoryimport ( \"github.com/yuraxdrumz/ports-and-adapters-golang/internal/app/cart/structs\")type Port interface { AddItemToDB(item *structs.Item) (string, error) RemoveItemFromDB(itemID string) (bool, error)} I like to use the repository design pattern to abstract database interaction.I already wrote about it here. Let’s see an example of the port for the cart.cart/ports.go 12345678package cartimport \"github.com/yuraxdrumz/ports-and-adapters-golang/internal/app/cart/structs\"type Port interface { Add(item structs.Item) error Remove(itemID string) error} Driver Portsdriver/ports.go 123456package driver// Port - how an adapter can use the apptype Port interface { Run()} Now that we finished the ports part, lets look at the adapters.We will first create simple adapters, like in memory repository adapter and a console warehouse adapter so that we can start using the app.Afterwards, we will create an sql adapter to show how Ports and Adapters come to our advantage. Adapters In PracticeDriven Adapterswarehouse/console.go 1234567891011121314151617181920212223242526272829package warehouseimport ( \"errors\")// our console warehouse struct with all the adapters it uses, in this case with the console adapter we dont need any dependenciestype ConsoleWarehouse struct {}// a new console warehouse factoryfunc NewConsoleWarehouse() *ConsoleWarehouse { return &amp;ConsoleWarehouse{}}// our implementation of the port for console warehousefunc (w *ConsoleWarehouse) CheckIfAvailable(itemID string) (bool, error) { if itemID == \"0\" { return false, errors.New(\"item is not available\") } return true, nil}// our implementation of the port for console warehousefunc (w *ConsoleWarehouse) RemoveItemFromWarehouse(itemID string) (bool, error) { if itemID == \"0\" { return false, errors.New(\"item cannot be removed from warehouse, please check again later\") } return true, nil} cartRepository/inMemory.go 123456789101112131415161718192021222324252627282930package cartrepositoryimport ( \"errors\" \"github.com/yuraxdrumz/ports-and-adapters-golang/internal/app/cart/structs\")// in memory repository adapter, we dont have any dependencies on other adapters so its emptytype InMemoryRepository struct {}// new in memory repository factoryfunc NewInMemoryRepository() *InMemoryRepository { return &amp;InMemoryRepository{}}// our implementation of the portfunc (r *InMemoryRepository) AddItemToDB(item *structs.Item) (bool, error) { if item.Id == \"0\" { return false, errors.New(\"cannot add item to db\") } return true, nil}// our implementation of the portfunc (r *InMemoryRepository) RemoveItemFromDB(itemID string) (bool, error) { if itemID == \"0\" { return false, errors.New(\"item cannot be removed, please check again later\") } return true, nil} Lets see the cart adapter with Add implemented cart/cart.go 12345678910111213141516171819202122232425262728293031323334353637383940414243package cartimport ( \"github.com/yuraxdrumz/ports-and-adapters-golang/internal/app/cart/structs\" \"github.com/yuraxdrumz/ports-and-adapters-golang/internal/pkg/adapters/out/cartRepository\" \"github.com/yuraxdrumz/ports-and-adapters-golang/internal/pkg/adapters/out/warehouse\" \"sync\")type Cart struct { wh warehouse.Port repo cartrepository.Port mutex sync.Mutex}func NewCart(wh warehouse.Port, repo cartrepository.Port) *Cart { return &amp;Cart{ wh: wh, repo: repo, }}func (c *Cart) Add(item *structs.Item) error { c.mutex.Lock() defer c.mutex.Unlock() isAvailable, err := c.wh.CheckIfAvailable(item.Id) if err != nil { return err } if isAvailable { _, err = c.repo.AddItemToDB(item) if err != nil { return err } } return nil}func (c *Cart) Remove(itemID string) error { return nil} At this point we have our cart, warehouse and repository ports and adapters implemented, all these ports are driven, because our app uses them internally.Now, we need a driver adapter. Let’s create a cli adapter which receives the item, id and description from the cli. Driver Adapterdriver/cli.go 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package driverimport ( \"github.com/sirupsen/logrus\" \"github.com/yuraxdrumz/ports-and-adapters-golang/internal/app/cart\" \"github.com/urfave/cli/v2\" \"github.com/yuraxdrumz/ports-and-adapters-golang/internal/app/cart/structs\" \"os\")// CliAdapter - struct with necessary ports to runtype CliAdapter struct { ca cart.Port}// NewCliAdapter - create a new instance of NewCliAdapter with passed implementationsfunc NewCliAdapter(ca cart.Port) *CliAdapter { return &amp;CliAdapter{ca: ca}}// Run - initializes cli adapter runfunc (in *CliAdapter) Run() { app := &amp;cli.App{ Name: \"cart\", Usage: \"handle cart from cli\", Flags: []cli.Flag{ &amp;cli.StringFlag{ Name: \"item\", Usage: \"item to add\", Required: true, }, &amp;cli.StringFlag{ Name: \"description\", Usage: \"description for the item\", Required: true, }, &amp;cli.StringFlag{ Name: \"id\", Usage: \"item id\", Required: true, }, }, Action: func(c *cli.Context) error { item := &amp;structs.Item{ Name: c.String(\"item\"), Id: c.String(\"id\"), Description: c.String(\"description\"), } err := in.ca.Add(item) if err != nil { logrus.WithField(\"error\", err.Error()).Error(\"couldn't add item\") return nil } logrus.WithFields(logrus.Fields{ \"name\": item.Name, \"description\": item.Description, \"id\": item.Id, }).Info(\"Added new item\") return nil }, } err := app.Run(os.Args) if err != nil { logrus.Fatal(err) }} Now lets glue everything together in main.go main.go 1234567891011121314151617181920212223package mainimport ( \"github.com/yuraxdrumz/ports-and-adapters-golang/internal/app/cart\" driver \"github.com/yuraxdrumz/ports-and-adapters-golang/internal/pkg/adapters/in\" \"github.com/yuraxdrumz/ports-and-adapters-golang/internal/pkg/adapters/out/warehouse\" \"github.com/yuraxdrumz/ports-and-adapters-golang/internal/pkg/adapters/out/cartRepository\")func main() { // declare all ports var ca cart.Port var wh warehouse.Port var re cartrepository.Port var in driver.Port wh = warehouse.NewConsoleWarehouse() re = cartrepository.NewInMemoryRepository() ca = cart.NewCart(wh, re) in = driver.NewCliAdapter(ca) in.Run()} Now, if we run go run main.go, we will get 1234567891011121314151617❯ go run main.goNAME: cart - handle cart from cliUSAGE: main [global options] command [command options] [arguments...]COMMANDS: help, h Shows a list of commands or help for one commandGLOBAL OPTIONS: --item value item to add --description value description for the item --id value item id --help, -h show help (default: false)FATA[0000] Required flags &quot;item, description, id&quot; not set exit status 1 Lets add an item with id 0 that is supposed to fail and with an id of 1 that is supposed to succeed 1234❯ go run main.go --item bicycle --description &quot;used as trasnport&quot; --id 1INFO[0000] Added new item description=&quot;used as trasnport&quot; id=1 name=bicycle❯ go run main.go --item bicycle --description &quot;used as trasnport&quot; --id 0ERRO[0000] couldn&apos;t add item error=&quot;item is not available&quot; It worked! Now, let’s create a new sql adapter for the repository cartRepository/sqlite.go 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package cartrepositoryimport ( \"database/sql\" \"errors\" \"github.com/sirupsen/logrus\" \"github.com/yuraxdrumz/ports-and-adapters-golang/internal/app/cart/structs\" _ \"github.com/mattn/go-sqlite3\")// in memory repository adapter, we dont have any dependencies on other adapters so its emptytype SQLiteRepository struct { db *sql.DB}// new in memory repository factoryfunc NewSQLiteRepository() *SQLiteRepository { db, err := sql.Open(\"sqlite3\", \"database/items.db\") if err != nil { logrus.Fatal(\"Couldn't initialize sqlite db\") } _, err = db.Exec(\"create table if not exists items (id string, name text, description text)\") if err != nil { logrus.WithField(\"error\", err.Error()).Fatal(\"Couldn't create initial items table\") } return &amp;SQLiteRepository{ db: db, }}// our implementation of the portfunc (r *SQLiteRepository) AddItemToDB(item *structs.Item) (string, error) { if item.Id == \"0\" { return \"\", errors.New(\"cannot add item to db\") } tx, _ := r.db.Begin() stmt, _ := tx.Prepare(\"insert into items (id, name, description) values (?,?,?)\") _, err := stmt.Exec(item.Id, item.Name, item.Description) if err != nil { return \"\", err } err = tx.Commit() if err != nil { return \"\", err } return \"random id\", nil}// our implementation of the portfunc (r *SQLiteRepository) RemoveItemFromDB(itemID string) (bool, error) { if itemID == \"0\" { return false, errors.New(\"item cannot be removed, please check again later\") } return true, nil} Lets change our main.go 123456789101112131415161718192021222324package mainimport ( \"github.com/yuraxdrumz/ports-and-adapters-golang/internal/app/cart\" driver \"github.com/yuraxdrumz/ports-and-adapters-golang/internal/pkg/adapters/in\" \"github.com/yuraxdrumz/ports-and-adapters-golang/internal/pkg/adapters/out/warehouse\" \"github.com/yuraxdrumz/ports-and-adapters-golang/internal/pkg/adapters/out/cartRepository\")func main() { // declare all ports var ca cart.Port var wh warehouse.Port var re cartrepository.Port var in driver.Port wh = warehouse.NewConsoleWarehouse() //re = cartrepository.NewInMemoryRepository() re = cartrepository.NewSQLiteRepository() ca = cart.NewCart(wh, re) in = driver.NewCliAdapter(ca) in.Run()} Lets run our main again 123456❯ go run main.go --item bicycle --description &quot;used as trasnport&quot; --id 1INFO[0000] Added new item description=&quot;used as trasnport&quot; id=1 name=bicycle❯ go run main.go --item bicycle --description &quot;used as trasnport&quot; --id 0ERRO[0000] couldn&apos;t add item error=&quot;item is not available&quot;❯ go run main.go --item bicycle --description &quot;used as trasnport&quot; --id 11INFO[0000] Added new item description=&quot;used as trasnport&quot; id=11 name=bicycle It worked and all the data was saved.By now you should be able to add a new adapter to any port, either driver or driven and interchange them without touching other parts of the system. A cool thing in Goland is we can easily see who implements our ports and reuse them by calling the adapter we want in main.go SummaryWe saw what is the Ports and Adapters pattern and how it is implemented in Golang.We created an initial warehouse, cart, cartRepository and driver(in) ports.Later on, we added each port’s adapter(implementation).Afterwards, we glued everything in main.go.By Using this architectural pattern we saw how components can interchange on any functionality.","link":"/2020/02/01/Implementing-Ports-and-Adapters/"},{"title":"Authentication Middleware in Elixir","text":"One of the first projects I was working on in Elixir was an API gateway. Like everyone else, I saw Pheonix, which is a cool framework for building web servers which is similar to Express.js, but, for my use case, I wanted raw performance for the API gateway, and one of its features was to have basic authentication as well as bearer authentication for json web tokens. One way to achieve this was using Plugs, which are built in the language. A plug is similar to a middleware in Express.js, it accepts input, does some manipulation and either halts the request or passes it on. In this post I will show how I implemented an authentication plug in Elixir. Router with plugsLets first look at how plugs look in a cowboy based router. 1234567891011121314151617defmodule Gateway.MainRouter do use Plug.Router use Plug.ErrorHandler import Plug.Conn plug(:match) plug(:fetch_query_params) plug(Plug.RequestId) plug(:dispatch) match _ do conn |&gt; put_resp_content_type(\"application/json\") |&gt; send_resp(200, \"Found\") endend We have our use statements as well as our import for Plug.Conn. Lines 7,8,9 and 10 are plugs. Every request that comes in passes through each of the plugs before it gets to the route matches. This is useful because we want to build an authentication plug that we can reuse later on and that can be plugged wherever. How to build a plugLets first see how to build a plug, from the plug documentation: 12345678910111213defmodule MyPlug do import Plug.Conn def init(options) do options end def call(conn, _opts) do conn |&gt; put_resp_content_type(\"text/plain\") |&gt; send_resp(200, \"Hello world\") endend Each plug must have 2 methods, one is init, which allows passing options at compile time to the plug and the other, the call method with the connection received. The call method must either halt the request or return the conn object. What do we need for authorizationLooks pretty simple, now lets think about the authorization we need before we start writing it. We must accept basic auth for server to server communcation We must accept jwt(bearer) auth for client to server communication If no auth header return 401 Defining our 401Before we start, lets define our 401, if its hit we halt and return 401: 1234567891011defmodule Plug.Auth do defp send_401( conn, data \\\\ %{message: \"Please make sure you have authentication header\"} ) do conn |&gt; put_resp_content_type(\"application/json\") |&gt; send_resp(401, Poison.encode!(data)) |&gt; halt endend Defining the call methodNow that we took care of the not authorized lets see how we implement the auth itself. The call method will extract auth header and call authenticate: 12345def call(%Plug.Conn{request_path: _path} = conn, _opts) do conn |&gt; get_auth_header |&gt; authenticateend Getting the authorization headerWe try get the auth header and call authenticate. If you ask where is the if/else/try/catch/send_401, Elixir has pattern matching and guard clauses we can leverage to avoid all the boilerplate of defensive programming to keep our focus on our use case.Lets define the get_auth_header method: 12345678defmodule Plug.Auth do defp get_auth_header(conn) do case get_req_header(conn, \"authorization\") do [token] -&gt; {conn, token} _ -&gt; {conn} end endend The get_req_header is from the Plug.Conn and allows getting the authorization header. We will return a tuple with the token if it exists or a tuple with the conn itself. AuthenticatingNow, the interesting part, after get_auth_header we call authenticate. Lets leverage Elixir’s power to extract what we need: 12345678910111213141516171819202122232425262728293031323334defmodule Plug.Auth do @secret \"my super secret\" @alg \"HS256\" @signer Joken.Signer.create(@alg, @secret) defp authenticate({conn, \"Bearer \" &lt;&gt; jwt}) do case Joken.verify(jwt, @signer) do {:ok, claims} -&gt; assign(conn, :user, claims) {:error, err} -&gt; send_401(conn, %{error: err}) end end defp authenticate({conn, \"Basic \" &lt;&gt; token}) do [username, password] = token |&gt; Base.decode64!(padding: false) |&gt; String.split(\":\") case Cache.get(\"users:#{username}\") do nil -&gt; send_401(conn, \"User does not exist\") %User{name: username, password: salted_password} -&gt; case Bcrypt.verify_pass(password, salted_password) do true -&gt; assign(conn, :user, %{name: username}) false -&gt; send_401(conn, \"Password is incorrect\") end end end defp authenticate(_) do send_401(conn) endend We defined 3 authenticate methods. If the get_auth_header returned a tuple with only conn in it, it means we have no auth header, so we call send_401, any other header returned that does not contain basic or bearer receives 401 as well. The other two methods are either Basic or Bearer. Elixir allows us to pattern match on binary strings, because our auth strings always start with Basic or Bearer we can match on them using the start_of_string &lt;&gt; rest_of_string syntax. If we hit the bearer auth, we check that the jwt matches, if the basic auth is hit, we compare it to what we have in our cache/db. If auth passes, the assign username/claims is called on the conn object and then returns it. Connecting it all togetherNow, lets go back to our initial router and add our plug to it: 123456789101112131415161718defmodule Gateway.MainRouter do use Plug.Router use Plug.ErrorHandler import Plug.Conn plug(:match) plug(Plug.Auth) &lt;- our auth plug plug(:fetch_query_params) plug(Plug.RequestId) plug(:dispatch) match _ do conn |&gt; put_resp_content_type(\"application/json\") |&gt; send_resp(200, \"Found\") endend Now we have an authenticated app! Full exampleHere is the full module, dont forget to add Joken or some other jwt checking library and replace Cache for basic auth with your own implementation: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263defmodule Plug.Auth do import Plug.Conn @secret \"my super secret\" @alg \"HS256\" @signer Joken.Signer.create(@alg, @secret) def init(opts) do opts end defp authenticate({conn, \"Bearer \" &lt;&gt; jwt}) do case Joken.verify(jwt, @signer) do {:ok, claims} -&gt; assign(conn, :user, claims) {:error, err} -&gt; send_401(conn, %{error: err}) end end defp authenticate({conn, \"Basic \" &lt;&gt; token}) do [username, password] = token |&gt; Base.decode64!(padding: false) |&gt; String.split(\":\") case Cache.get(\"users:#{username}\") do nil -&gt; send_401(conn, \"User does not exist\") %User{name: username, password: salted_password} -&gt; case Bcrypt.verify_pass(password, salted_password) do true -&gt; assign(conn, :user, %{name: username}) false -&gt; send_401(conn, \"Password is incorrect\") end end end defp authenticate(_) do send_401(conn) end defp send_401( conn, data \\\\ %{message: \"Please make sure you have authentication header\"} ) do conn |&gt; put_resp_content_type(\"application/json\") |&gt; send_resp(401, Poison.encode!(data)) |&gt; halt end defp get_auth_header(conn) do case get_req_header(conn, \"authorization\") do [token] -&gt; {conn, token} _ -&gt; {conn} end end def call(%Plug.Conn{request_path: _path} = conn, _opts) do conn |&gt; get_auth_header |&gt; authenticate endend SummaryWe learned what are plugs in Elixir, later on we saw how to implement a plug ourselves. Afterwards, we defined our authentication requirements and last, we implemented the authentication, leveraging Elixir’s pattern matching capabilities.","link":"/2019/07/03/Authentication-Middleware-In-Elixir/"},{"title":"How the Event Loop Really Works","text":"After writing in Node.js for 3 years, I felt that the majority of the posts about it were lacking deeper knowledge, especially about the event loops internals, so, I decided to dig deep and give you my insights. Node.js seems like a simple beast to handle but in reality, there are a lot of layers of abstraction behind it. From one perspective, this abstraction is great, but, like everything else in software, abstraction comes at a cost.The purpose of this post is to show you a broader picture of how the Node core handles the event loop, what it really is and how it coexists with the JS runtime. Lets start with asking a few simple questions and then answer them with detailed examples. What is the event loop? What are the phases of the event loop? What are the differences between the phases of the event loop and Node.js? How is it implemented in practice? Is Node.js really single threaded? Why do we have a thread pool if we already have the event loop that does all the async operations? What is the event loop??From the Node.js website, summarized: Node.js is an event-based platform. This means that everything that happens in Node is the reaction to an event. Abstracted away from the developer, the reactions to events are all handled by a library called libuv. libuv is cross-platform support library which was originally written for Node.js. It is designed around the event-driven asynchronous I/O model. libuv provides 2 mechanisms. One is a called an event loop, which runs on one thread and the other mechanism is called a thread pool. It is important to note that the event loop and the JS runtime run on the same thread and the thread pool is initiated with 4 threads by default. Also, libuv can be used not only in Node.js, that is why I will show the differences between the event loop itself and the event loop in Node.js context. What are the phases in libuv?The phases of libuv are: update loop time - it is the starting point to know time from now on check loop alive - Loop is considered alive if it has active and ref’d handles, active requests or closing handles. Run due timers - the timers we pass to libuv, that are past the definition of now, we defined in the first phase, are getting called. The timers are held in a min heap data structure, for efficiency. If the first timer is not expired than we know that all others below it were created later in time. Call pending callbacks - There are cases in which the callbacks are deffered to next iteration, this is where they are called. Run idle handles - internal Run prepare handles - internal Poll for IO - this is where the loop handles any incoming TCP connections. The time for how long the loop is waiting in each iteration will be explained later on. Run check handles - All check handles are run here, for example setImmediate if we are in the context of Node.js Run close callbacks - all .on('close') events are called here, if something happened we want that the timers+poll+check callbacks will complete and afterwards the close callbacks are called, for example, the socket.destroy() in Node.js. Repeat The visual diagram of the phases looks like this, do not worry if you still do not get the full picture, we still have things to cover! What are the differences between the phases of libuv and Node.jsNode.js uses all of the above mentioned phases and adds 2 more: the process.nextTick and microTaskQueue. the differences in Node.js are: Process.nextTick() is not technically part of the event loop. Instead, the nextTickQueue will be processed after the current operation is completed, regardless of the current phase of the event loop. Here, an operation is defined as a transition from the underlying C/C++ handler, and handling the JavaScript that needs to be executed.Any time you call process.nextTick() in a given phase, all callbacks passed to process.nextTick() will be resolved before the event loop continues. This can create some bad situations because it allows you to “starve” your I/O by making recursive process.nextTick() calls, which prevents the event loop from reaching the poll phase. In the context of native promises, a promise callback is considered as a microtask and queued in a microtask queue which will be processed right after the next tick queue. It is also dangerous as recursive calls to promises in theory will never reach the next tick. When the event loop enters the poll phase and there are no timers scheduled, one of two things will happen:If the poll queue is not empty, the event loop will iterate through its queue of callbacks executing them synchronously until either the queue has been exhausted, or the system-dependent hard limit is reached.If the poll queue is empty, one of two more things will happen:If scripts have been scheduled by setImmediate(), the event loop will end the poll phase and continue to the check phase to execute those scheduled scripts.If scripts have not been scheduled by setImmediate(), the event loop will wait for callbacks to be added to the queue, then execute them immediately.Once the poll queue is empty the event loop will check for timers whose time thresholds have been reached. If one or more timers are ready, the event loop will wrap back to the timers phase to execute those timers’ callbacks Before we continue, it is important to note that all phases we mentioned(timers, pending, poll, close) always call 1 callback which is a macro operation that is sent to JS land and afterwards the nextTickQueue + microTaskQueue are run, no matter where we are in the event loop. Each macro operation combined with the nextTickQueue and microTaskQueue is called an event loop tick. Here is the diagram for Node.js event loop, notice there is not much difference except the 2 queues we talked about(nextTickQueue and microTaskQueue). Did you notice something interesting we learned about the callback queues? Every one talks about a callback queue but in reality each phase has its own callback queue. How is the event loop implemented in practice?Here is the c++ code from the libuv github, I am sure you can figure the phases out by yourself and if you look carefully, you will see that the timeout for the poll phase is calculated. Also, we have a special mode of RUN_ONCE which dictates that if no callbacks were called in current poll phase, it means we have no work to do except the timers, so just update time and run timers phase before exiting. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950int uv_run(uv_loop_t* loop, uv_run_mode mode) { int timeout; int r; int ran_pending; r = uv__loop_alive(loop); if (!r) uv__update_time(loop); while (r != 0 &amp;&amp; loop-&gt;stop_flag == 0) { uv__update_time(loop); uv__run_timers(loop); ran_pending = uv__run_pending(loop); uv__run_idle(loop); uv__run_prepare(loop); timeout = 0; if ((mode == UV_RUN_ONCE &amp;&amp; !ran_pending) || mode == UV_RUN_DEFAULT) timeout = uv_backend_timeout(loop); uv__io_poll(loop, timeout); uv__run_check(loop); uv__run_closing_handles(loop); if (mode == UV_RUN_ONCE) { /* UV_RUN_ONCE implies forward progress: at least one callback must have * been invoked when it returns. uv__io_poll() can return without doing * I/O (meaning: no callbacks) when its timeout expires - which means we * have pending timers that satisfy the forward progress constraint. * * UV_RUN_NOWAIT makes no guarantees about progress so it's omitted from * the check. */ uv__update_time(loop); uv__run_timers(loop); } r = uv__loop_alive(loop); if (mode == UV_RUN_ONCE || mode == UV_RUN_NOWAIT) break; } /* The if statement lets gcc compile it to a conditional store. Avoids * dirtying a cache line. */ if (loop-&gt;stop_flag != 0) loop-&gt;stop_flag = 0; return r;} Let’s see how the timeout for the poll phase(waiting on TCP connections) is calculated.To break it down for you it goes like this: If the stop flag is not 0, return 0 If we dont have active handles or requests, return 0 If the idle and pending queues are not empty return 0 If we have closing handles return 0 Otherwise return the next timeout from the loop What this means is that the loop will not wait in poll phase if not neccessary, but if we have a timer, then it will wait in poll phase for the time of that timer in order to wrap to the timers phase instead of just doing meaningless iterations. 123456789101112131415161718int uv_backend_timeout(const uv_loop_t* loop) { if (loop-&gt;stop_flag != 0) return 0; if (!uv__has_active_handles(loop) &amp;&amp; !uv__has_active_reqs(loop)) return 0; if (!QUEUE_EMPTY(&amp;loop-&gt;idle_handles)) return 0; if (!QUEUE_EMPTY(&amp;loop-&gt;pending_queue)) return 0; if (loop-&gt;closing_handles) return 0; return uv__next_timeout(loop);} Is Node.js single threaded?Well, as we said previously, the event loop runs on the same thread as the JS runtime, which means, that we have one execution thread, so by definition, Node.js is single threaded. Essentially, when the JS stack is empty of user code(all sync code we defined in our scripts), the event loop runs a tick. Important thing to note - slow sync code can halt the system to the ground at scale, due to the shared thread of the JS runtime with the event loop. More concrete, the context switch to c++ land cannot happen until the JS stack is empty. If you try to do recursive operations in different queues(different stages of the event loop), even the setImmediate, it will always run on later ticks, remember the 1 macro per tick we talked about? This is it. But, microTaskQueue and nextTickQueue that are defined in JS land, will run recursively on the same tick, because we learned that the event loop will only run when all JS code is executed and the 2 queues above are simply javascript. Why do we have a thread pool if we already have the event loop that does all the async operations?This is an interesting part of Node.js land, because it is single threaded, like we already answered, but its also not! It all depends from what perspective you look at it. We do have one execution thread, but we also have the thread pool that comes along with the event loop. Taken from the libuv website: Whenever possible, libuv will use asynchronous interfaces provided by the OS, avoiding usage of the thread pool. The same applies to third party subsystems like databases. The event loop follows the rather usual single threaded asynchronous I/O approach: all (network) I/O is performed on non-blocking sockets which are polled using the best mechanism available on the given platform: epoll on Linux, kqueue on OSX and other BSDs, event ports on SunOS and IOCP on Windows. What this basically means is that all operating systems have some sort of non-blocking IO handling we can leverage. We can subscribe to all events regarding new TCP connections, incoming data from TCP connection and etc. But what about crypto stuff, DNS resolving, file system operations? They are supposed to be non blocking as well, right? Well, Not all the types of I/O can be performed using async implementations. Even on the same OS platform, there are complexities in supporting different types of I/O. Typically, network I/O can be performed in a non-blocking way using epoll, kqueue, event ports and IOCP, but the file I/O is much more complex. Certain systems, such as Linux does not support complete asynchrony for file system access. And there are limitations in file system event notifications/signaling with kqueue in MacOS. Ok, we figured out that fs operations are problematic and must be done sync, but why DNS resolving must be sync too? To resolve DNS you must read the hosts file first on the host machine. We must use the fs for that and you get it by now…Regarding the crypto, Cryptography on the most basic level is performing calculations, and there is no way to to calculations asynchronously on the same thread. For all the reasons above, we have a thread pool. When you send an async fs operation or an async crypto/dns resolve callbacks to the event loop, it gets picked up by the thread pool and resolved on a different thread synchronously. ExamplesNow, after we answered all the questions, lets look at some examples:We will call the crypto.pbkdf2 async implementation. I added a comment of how the results look in the previous example with default 4 threads that took 9xx ms completion time Example 1:1234567891011const { pbkdf2 } = require('crypto');const start = Date.now();const doExpensiveHashing = () =&gt; { pbkdf2('pwd', 'salt', 100000, 512, 'sha512', () =&gt; console.log(`Done in ${Date.now() - start}ms`) );};doExpensiveHashing(); // Done in 937msdoExpensiveHashing(); // Done in 942msdoExpensiveHashing(); // Done in 943msdoExpensiveHashing(); // Done in 943ms now set the environment variable UV_THREADPOOL_SIZE=1 and run the code again, 1234567891011const { pbkdf2 } = require('crypto');const start = Date.now();const doExpensiveHashing = () =&gt; { pbkdf2('pwd', 'salt', 100000, 512, 'sha512', () =&gt; console.log(`Done in ${Date.now() - start}ms`) );};doExpensiveHashing(); // Done in 658msdoExpensiveHashing(); // Done in 1321msdoExpensiveHashing(); // Done in 1982msdoExpensiveHashing(); // Done in 2637ms We saw that we get sequential execution because we have only 1 thread. If you try the sync implementation we will simply block our thread, but the thread pool allows us to do all the non trivial crypto work on another thread and keep our business rolling! Isn’t that cool? Example 2:Before we start, important note, if we set setTimeout to 0, it will be 1ms. Who will execute first? According to what we saw, it will be setImmediate because the timer has not expired yet, but try and run it several times: 12setTimeout(()=&gt;console.log('setTimeout'),0)setImmediate(()=&gt;console.log('setImmediate')) First time: 12setImmediatesetTimeout Second time: 12setTimeoutsetImmediate The interesting part here is that we know that when the event loop runs, it checks if the timer expired, which in our case is 1ms, which is a long time for a cpu, and the setImmediate will run only on next tick after we defined it and only after poll phase!. If the timer expired on start of next tick it will be called first, if not, the setImmediate will be called first and setTimeout afterwards. Example 3:Now, lets add an IO cycle 123456const fs = require('fs')fs.readFile('test.txt', ()=&gt;{ setTimeout(()=&gt;console.log('setTimeout'),0) setImmediate(()=&gt;console.log('setImmediate'))}) The fs sends the readFile to our thread pool, when the operation finishes in poll phase, it registers the setTimeout and setImmediate and as we saw, the check phase always comes after the poll phase, so our setImmediate is guaranteed to execute first. Example 4:Recursive operations in the event loop: take this example: 123456789101112131415161718const express = require('express')const app = express()app.use((req,res,next)=&gt;{ res.json('ok')})function compute(){ for(i=0;i&lt;1e3;i++){ console.log(`computing i=${i}...`) } setImmediate(compute) // Promise.resolve().then(compute) // process.nextTick(compute)}app.listen(3000, ()=&gt;console.log('listening on port 3000'))compute() Will our web server answer our requests?Remember we said that recursive operations are dangerous on microTaskQueue and nextTickQueue in JS land? Well, thats true, which means, that a recursive call to setImmediate will not be immediate(1 macro per tick, remember?), but split to next tick after each invocation and that means that we pass iterations through poll phase which is responsible for our IO. To answer the question - Yes, we will answer the web requests! Example 5:Now, lets see with Promise.resolve().then(compute) 123456789101112131415161718const express = require('express')const app = express()app.use((req,res,next)=&gt;{ res.json('ok')})function compute(){ for(i=0;i&lt;1e3;i++){ console.log(`computing i=${i}...`) } // setImmediate(compute) Promise.resolve().then(compute) // process.nextTick(compute)}app.listen(3000, ()=&gt;console.log('listening on port 3000'))compute() The native implementation of promises work with the microTaskQueue, which will run in recursion indefinitely and our web server will never answer! Fun fact - With native promises we could implement long running promises that do recursions, and would never continue to next ticks - so fs operations and other network IO and phases from c++ land will never get execution time! For these reasons, Bluebird and other promise libraries implemented the resolve and reject to be in setImmediate which will never starve the IO! The process.nextTick should be obvious by now. Example 6:Last one before we wrap up: 123456789101112131415161718Promise.resolve().then(() =&gt; console.log('promise1 resolved'));Promise.resolve().then(() =&gt; console.log('promise2 resolved'));Promise.resolve().then(() =&gt; { console.log('promise3 resolved'); process.nextTick(() =&gt; console.log('next tick inside promise resolve handler'));});Promise.resolve().then(() =&gt; console.log('promise4 resolved'));Promise.resolve().then(() =&gt; console.log('promise5 resolved'));setImmediate(() =&gt; console.log('set immediate1'));setImmediate(() =&gt; console.log('set immediate2'));process.nextTick(() =&gt; console.log('next tick1'));process.nextTick(() =&gt; console.log('next tick2'));process.nextTick(() =&gt; console.log('next tick3'));setTimeout(() =&gt; console.log('set timeout'), 0);setImmediate(() =&gt; console.log('set immediate3'));setImmediate(() =&gt; console.log('set immediate4')); Do not be startled! Lets do what we always do in software engineering - break it down Promise.resolve - native implementation and on js land, it will run last after nextTickQueue setImmediate runs after poll phase setTimeout runs on timers phase process.nextTick must be exhausted before continuing to next tick Now, nextTick is run on current tick and must be exhausted like we said, so it has to go first, because it runs on JS land regardless of where we are on the event loop! After the nextTick we have the microTaskQueue which like we said previously, is run last after nextTickQueue so it runs.We have a process.nextTick inside the callback of a micro tick. That one we did not cover on purpose! What do you think will happen? Like we said, the nextTickQueue must be exhasted on each iteration, and we added a callback to the queue, so it wraps back to nextTickQueue and runs the callbacks again.After all those steps we finally can start a new tick and by this time our timer has elapsed, so our setTimeout gets executed, our setImmediate is last and the result is: next tick1 next tick2 next tick3 promise1 resolved promise2 resolved promise3 resolved promise4 resolved promise5 resolved next tick inside promise resolve handler set timeout set immediate1 set immediate2 set immediate3 set immediate4 SummaryWe started by asking a few basic questions in Node.js, like: how does its internals look like and how does it operate? Later on, we tried to answer each of those questions as deep as possible. We learned that after all the abstractions, it is a (not so simple) while loop which enables us to build advanced systems and build web servers without worrying about concurrency control or multi threading. Finally, we saw some examples that seem trivial at first, but after our dive to Node core, we were able to answer them.","link":"/2019/06/09/Node-JS-Event-Loop-0/"},{"title":"What Are Microservices","text":"After having written and implemented several microservice architectures, I wanted to have a go at explaining microservices from my point of view and share my insights. In this post I will explain what are microservices, what are their pros and cons, how they communicate and the different approaches towards building microservices. What are microservicesThere are a lot of explanations out in the wild regarding what are microservices. Everyone keeps mentioning loose coupling and independant deployments, but in my experience, that is not always true. Sometimes, you will have to deploy several microservices together because they are coupled, a.k.a, orchestration. In my point of view, microservices need to control their own copy of the application data and they should be self contained, meaning, the business logic they handle should only be tied to a specific microservice and to it alone. Microservices Pros The complexity of the app is too large for a single team/repo/server to handle Smaller codebase for each team to work on and if possible deploy independently Faster deployments Safer deployments, we avoid breaking the entire app, each of the services should be self contained Freedom in choosing tech stack, language, database as every service is independant and knows how to speak to the other services Microservices Cons Having a lot of small services means a lot of deployments from various teams, you will need strong CI/CD tools and best practices to keep up. Integration testing and integration in general will be tougher, as in the real world some services will have to deploy together, so orchestration is needed. Each service having its own copy of the data means your data will be eventually consistent at best, distributed transactions are usually a performance bottleneck Identifying which services belong together and which are not is hard. Communication is a pain in the butt. Do we need gRPC or Websockets or HTTP or Event bus? A lot of factors to consider Microservices communicationMicroservices can communicate with each other either sync or async: Sync - HTTP 1.1Pros: Most tutorials, examples and implementations are in HTTP, so it is easier to pick it up and get started Cons: For applications that need to do long running work and/or react to certain events, this type of design can quickly become a bottleneck Async - gRPC, websockets, eventsPros: Makes more sense when the application does a lot of background work and with the use of gRPC/websockets notifies the user when it is done. Cons: Tougher to manage, as each event can trigger a lot of reactions which cascade to more events being created. Approaches to building microservicesSide CarExpose the services directly. Each service will have a side-car, which is a service attached to each of your services, hence side-car, which does all the logging, forwarding, service discovery, ssl termination, authentication and authorization. Pros: Direct client/server communication, means better performance than dealing with extra layers Cons: Must conform to client communication mechanism. If the client uses HTTP 1.1 your services must use it too. API GatewayExpose an API Gateway, which will act as another layer between your services and the clients. Pros: Services can be hidden from the public internet, which means better security Internal communication can be whatever you like, because you have an extra translation layer Cons: Another layer to take care of Reduced performance because of said layer API ComposerThis acts as an API Gateway, but it does the requests to the services on behalf of the client and composes their responses as well. Pros: Removes the burden from the client of making multiple requests to the server, as the composer composes all the responses for the client Decouples the services even further as their communication is reduced Cons: Easy to leak functionality outside of services Extra layer to take care of Backend For FrontendThis acts like an API Composer, but you create a composer per client (mobile vs tv). This allows you to make different data requests to your inner services based on the client. For example you have a mobile client and a tv client, which both request data streams from your services. The data and bandwidth needs of your mobile and tv greatly differ, so instead of dealing with managing compatability with a lot of clients in a single composer, you create a composer per client. Pros: Allows each team to focus on their specific client needs without breaking the other composers. All the composers rely on the services layer. Allows the team that writes the front end to use the composer to their needs as long as the services layer can support their needs, hence the backend for frontend pattern. Cons: Easy to leak functionality outside of services Duplication of code between multiple composers Wrapping UpBefore rushing to build microservices, remember that there is a lot more to them. People say monolith like it is a bad word, when in reality, they talk about the code and that it is coupled, undocumented and not modular. It is perfectly fine and desired to run your code on a single server with a single database on a single platform, as long as you keep your code modular. Microservices are an evolutionary step you need to take as your business needs progress and become more demanding/complex to the point microservices are helpful and not a burden.","link":"/2019/11/05/what-are-microservices/"},{"title":"Switching From Node.js to Elixir","text":"After writing software for the past 6 years, I realized that the implementations that we write are far more important than the languages we choose, but, having a developer friendly, safe, strict language, can guide you towards better implementations. That is why I set out to search for a new programming language, that will replace my daily driver - Node.js. In a search for a new programming language, I asked myself two questions - what are the must-have features I expect from a programming language? and do its pros outweigh its cons?In this post, I will weigh some of Javascript’s pros and cons, later on, I will list the features, I believe, a programming language should have, afterwards, I will explain why I chose to learn Elixir and at last, I will weigh Elixir’s pros and cons. Javscript - The Pros Low development times - it is very easy to get started with Javascript The biggest community in the world - thousands of questions on stackoverflow and npm - the biggest package registry in the world. A lot of battle-tested frameworks and projects online - due to the popularity, there are a lot of open source projects out there. Hiring is easier than other languages due to the popularity. Runs on both the browser and the server - great for front end developers that want or need to make the switch. Javascript - The Cons Single threaded - If you have a long running sync operation, your entire application will halt. Memory leaks are hard to diagnose and will bite without large amounts of debugging and profiling. As easy as it is to start programming in Node.js, as hard it is to master it. The event loop, which I had covered extensively in my previous post, is not a simple beast to handle. The coupling to npm. I had several times where npm install failed in production because of some strange dependency not getting installed even though it was available in the registry. I even had published packages disappear into thin air. The hybrid approach - The JS community advocates that it is great that you can write the same thing in a lot of ways, both in the Object Oriented and Functional programming paradigms, but in reality, not having a unified approach to writing code leads to worse codebases, especially in large groups with tens of developers from different backgrounds. Do Javascript’s pros outweigh its cons ?After having to deal with nasty memory leaks, tough debugging sessions and data being mutated wherever and whenever, I decided to tilt towards no. The single threaded nature is both good and bad, the syntax changes so fast, I have seen callbacks wrapped in promises and awaited on using the new async await syntax and debugging in production can be a nightmare in large projects. What are some of the things I expect from a programming language Functional programming paradigm - Writing in a functional style for the past 3 years with Ramda.js has taught me a lot and I wanted a purely functional language. Switching to legacy projects written in an OO way always felt harder to reason about. I believe currying, functions as first class citizens and data immutability helps with the struggles of debugging. Language syntax - I wanted a simple to understand syntax that has no drastic changes throughout its lifespan. Compiled language - If everything compiled, I can sleep better at night. I am not expecting runtime errors to be unheard of, but, if I can minimize them, why not? Good standard library - To minimize the need for 3rd party modules. Strong community - For when I have a non trivial problem at hand. Concurrency - I want a language that can handle concurrency, without worrying about a single thread struggling to keep up or handling a threadpool and the accompanied mutexes/locks/deadlocks. After playing with a few languages, I stumbled across Elixir. What is Elixir ?from the elixir website Elixir is a dynamic, functional language designed for building scalable and maintainable applications. Elixir leverages the Erlang VM, known for running low-latency, distributed and fault-tolerant systems, while also being successfully used in web development and the embedded software domain. After reading the initial documentation, seeing that there is less than 10 open issues in the language’s github and writing a couple of projects for the past few months, I came back to my list and looked if Elixir has ticked all the boxes. Is it Functional? Is the syntax simple? inspired by Ruby, which is very easy to understand. Is is compiled? It is still dynamic typed though, so run time type errors will occur, but Elixir has guards and pattern matching for that. Does it have a good standard library? It has a detailed and well documented standard library. Is the community active? Elixir has its own forums, which are really active. Is it concurrent? All Elixir code runs inside lightweight threads of execution (called processes) that are isolated and exchange information via messages - Actor Model. Elixir - The Pros Highly concurrent - thanks to the Actor Model Functional - so we gain data immutability Scalable - distrubution of work across multiple nodes and communication is baked in the Erlang VM. Highly available - Due to their lightweight nature, it is not uncommon to have hundreds of thousands of processes running concurrently in the same machine. Isolation allows processes to be garbage collected independently, reducing system-wide pauses, and using all machine resources as efficiently as possible (vertical scaling). Pattern matching - Elixir has pattern matching and guard clauses. Elixir - The Cons Young - It will take time until Elixir matures and shapes to a full featured language. Functional programming paradigm is difficult to learn. Hiring will be a pain as there are not a lot of experts in this domain. Up until v1.9 it was very confusing on how to compile a release. Raw processing - If all you do is number crunching, maybe you are better of with a different language. Elixir puts an emphasis on being highly concurrent and fault tolerant, but there are better languages out there for raw processing power. Do Elixir’s Pros outweigh its cons ?Companies, like Pinterest and Discord use Elixir for their highly sensitive systems and are very happy with the results. The language has a great standard library, good community, great debugging tools, like remote host connect and real time process inspections. The Erlang VM utilizes all of the machines resources and provides a full featured and stable runtime. My answer for now, will be - yes.","link":"/2019/06/17/Switching-From-Node.js-To-Elixir/"},{"title":"User Space Scheduling","text":"A scheduler is a complex piece of software that is responsible for making sure cores are not idle if there are Threads that need work to be done.The fast switching of Threads, also called a context switch, is done by the scheduler and it gives us the illusion that all of our processes run in parallel.We have schedulers both in the Kernel of our favorite OS and in the user space, where our programs live and run.Some types of programming languages leverage one type of scheduling, while others leverage another.In this blog post, I will explain what a scheduler is and compare how scheduling affects different programming languages. Before we jump to the scheduling section, let’s refresh our memory with what Threads are. ThreadsA Thread is a set of instructions that waits to be called and there are two types of Threads: Kernel Threads (OS Threads) Green Threads (lightweight Threads) Kernel ThreadsEvery application you run creates a process and each process is given an initial Kernel Thread, which in turn can create more Kernel Threads.Kernel Threads can run concurrently by taking a turn on a single core, or in parallel each running at the same time on different cores.When a Thread receives running time on a core it executes its instructions until it is pulled of the core. Kernel Thread StatesA Kernel Thread can have three states Waiting - the Thread is stopped because it is waiting for a sync call (mutex), a system call or a disk. Runnable - the Thread is ready to be run on a core so it can execute its instructions Executing - the Thread is executing instructions on the core. Green ThreadsThese Threads only exist in our programs in the user space and are invisible to the Kernel.The handling and scheduling of these Threads occurs in user space as well, depending on the Threading model. The states of user space threads depend on how the scheduler that administers them is implemented. SchedulerThe scheduler is a component that selects which Thread to run next and there are two types of schedulers: PreemptiveThe scheduler decides when a Thread is to cease running and a new Thread is to resume running (context switching).Preemptive scheduling has to solve getting all kinds of software from all kinds of places to efficiently share a CPU. BenefitsThere is a degree of fairness to all Threads. DrawbacksBecause Threads get interrupted and switched, there is an overhead to store/restore the Thread’s state each context switch, which is very expensive computation wise. CooperativeA process does not stop running until it voluntarily decides to do so. BenefitsWorks well for processes designed to work together.The scheduler is much simpler to implement.There is no context switching so we get better performance. DrawbacksIf a program forgot to yield control back to the scheduler, other programs will not get a chance to run on that core. All major OS’s today use a preemptive type of scheduler.Windows 3.1 and MacOS 9 had a cooperative scheduler. Threading ModelsThere are different Threading models that explain how our Green Threads are linked to Kernel threads. 1-1When we call to create a Green Thread in our program, it invokes a system call to spawn an Kernel Thread.This type of Threading model does not need its own scheduler in user space.Examples would be: Java (JVM), C, Rust BenefitsNo need for a user space scheduler as we reuse the preemptive OS scheduler. DrawbacksCreating a large number of threads consumes a lot of system resources.The same drawbacks to context switching apply here to due using the same OS scheduler. N-1We have multiple Green Threads and only one Kernel Thread.This type of Threading model does need its own scheduler in user space.The prime example would be Node.js. BenefitsDue to running on a single Kernel Thread, there is no overhead thinking about race conditions and mutexes. DrawbacksCannot leverage multi core processors M-NWith this Threading model we create multiple Green Threads that run on multiple Kernel Threads.This type of Threading model does need its own scheduler in user space.Examples would be: RxJava, Akka, Go BenefitsLeveraging the best of both worlds DrawbacksYou need a really good implementation of a user space scheduler to make this threading model utilize the most out of your systemData races and sync issues can occur, so it is up to the developers to handle them. Kernel and User Space SchedulingWe said earlier that all major OS’s today use a preemptive type of scheduler, but it does not mean that all user space schedulers are preemptive as well. Preemptive Kernel and Cooperative User SpaceTake, for example Node.js, its event loop is actually a cooperative scheduler.All phases in the event loop are run only after the previous phase finished running, meaning it gave control back to the event loop.Another example for cooperative scheduling is Go, however we dont have to explicitly yield control back to its scheduler, as it does that by itself on each function invocation. Because we have the OS preemptive scheduler, we can leverage the cooperative scheduling in user space, thus achieving both performance and concurrency. Preemptive Kernel and Preemptive User SpacePreemptive scheduling is hard because we need to take into account context switches and synchronization primitives, however, there are a few languages like Erlang, Elixir and Haskell that use a preemptive scheduler in user space as well.Due to their functional nature, the languages above do not share memory between processes, so they dont need synchronization.The Erlang vm, for example, creates a preemptive scheduler per CPU core, allowing for maximum concurrency.Even if you block a Thread in your application with synchronous code, the vm will still give other Green Threads time to run. ConclusionWe saw what Threads are and how the scheduler uses them to not let the system idle if there is work to be done. Last, we saw how each of the Threading models affects how programming languages / runtimes work and how they are used.","link":"/2020/03/07/User-Space-Scheduling/"},{"title":"Microservices Epiphany","text":"The majority of the posts I see about microservices talk about the differences vs monoliths and how everyone, including myself, is rushing to build microservices in this fast paced world we live in. Recently, I read Implementing Domain Driven Design by Vaughn Vernon, which seemed unrelated to microservices at first but soon changed my perspective on things. What I experienced, like the title suggests, was an epiphany that I was building microservices wrong all along. In fact, I was building smaller monoliths, separated by a url subdomain. Head Explodes! In this short post, I will show a couple of symptoms that I found are a sign your microservices architecture might suffer in the long run. Symptoms of a monolithOne of the things I noticed after practicing DDD for a while, was that all the services I recently wrote were small monoliths. I mention DDD due to the fact that I understood some concepts, like bounded contexts, context maps and etc that helped me question my past design choices. The first symptom is that your services do not communicate. If you have a microservices architecture, you should have a mesh of interconnected components, either RESTful or evented. In fact, if you had the “joy” between deciding where to join responses, either at the gateway level or at the service level, risking over coupling, it means you have services with some boundaries and you reuse past implementations. Otherwise, you simply have a bunch of services under the same domain. Continuing the streakThe second symptom I noticed and the first thing that should have startled me a long time ago was a one rules them all database. My team and I had a legacy MongoDB Replica which was the only database we had and we tried building microservices while ignoring the database per service rule. Not only is it a single point of failure, but even worse, is the fact that, having everything stored in one place lures you in favor of adding just another feature that might not be related to a particular service and belongs in its own context. Of course this ends badly with monoliths bombed with 30 features each with non related behavior whatsoever and you excuse yourself with Why should I duplicate the data, I have everything right here. Obey the Rules! How Domain Driven Design helpedUnderstanding parts like where to set clear boundaries between services, defining a shared langauge with the domain experts, and seeking reuse together with careful design, led me to the understanding that things that might seem unrelated at first like DDD and microservices, have in fact so much in common. What I really mean is that when you looked at each component separately everything looked great, but when the entire infrastructure and services were charted together, things started to look disconnected and along came the epiphany accompanied with What have I done!? SummaryAfter all, there are hundreds, if not thousands of tutorials on how to build microservices out in the wild. So how come my team and I repeated the few things everyone warned us not to do. Maybe it was the laziness to refactor or the so-called “developers ego” or perhaps the notion of “it works so why bother changing?” Either way, learning something that seemed unrelated and applying it to something existing opened my eyes.","link":"/2019/07/16/Microservices-Epiphany/"}],"tags":[{"name":"DDD","slug":"DDD","link":"/tags/DDD/"},{"name":"Software Architecture","slug":"Software-Architecture","link":"/tags/Software-Architecture/"},{"name":"Context Maps","slug":"Context-Maps","link":"/tags/Context-Maps/"},{"name":"Node.js","slug":"Node-js","link":"/tags/Node-js/"},{"name":"Typescript","slug":"Typescript","link":"/tags/Typescript/"},{"name":"Clean Architecture","slug":"Clean-Architecture","link":"/tags/Clean-Architecture/"},{"name":"Uncle Bob","slug":"Uncle-Bob","link":"/tags/Uncle-Bob/"},{"name":"Programming Paradigms","slug":"Programming-Paradigms","link":"/tags/Programming-Paradigms/"},{"name":"Functional","slug":"Functional","link":"/tags/Functional/"},{"name":"Ports and Adapters","slug":"Ports-and-Adapters","link":"/tags/Ports-and-Adapters/"},{"name":"Golang","slug":"Golang","link":"/tags/Golang/"},{"name":"Hexagonal","slug":"Hexagonal","link":"/tags/Hexagonal/"},{"name":"Elixir","slug":"Elixir","link":"/tags/Elixir/"},{"name":"Javascript","slug":"Javascript","link":"/tags/Javascript/"},{"name":"Event Loop","slug":"Event-Loop","link":"/tags/Event-Loop/"},{"name":"libuv","slug":"libuv","link":"/tags/libuv/"},{"name":"Microservices","slug":"Microservices","link":"/tags/Microservices/"},{"name":"Design","slug":"Design","link":"/tags/Design/"},{"name":"Backend For Frontend","slug":"Backend-For-Frontend","link":"/tags/Backend-For-Frontend/"},{"name":"API Gateway","slug":"API-Gateway","link":"/tags/API-Gateway/"},{"name":"Sidecar Pattern","slug":"Sidecar-Pattern","link":"/tags/Sidecar-Pattern/"},{"name":"Actor Model","slug":"Actor-Model","link":"/tags/Actor-Model/"},{"name":"Concurrency","slug":"Concurrency","link":"/tags/Concurrency/"},{"name":"Parallelism","slug":"Parallelism","link":"/tags/Parallelism/"},{"name":"Threads","slug":"Threads","link":"/tags/Threads/"},{"name":"Scheduler","slug":"Scheduler","link":"/tags/Scheduler/"},{"name":"Green Threads","slug":"Green-Threads","link":"/tags/Green-Threads/"},{"name":"Kernel Threads","slug":"Kernel-Threads","link":"/tags/Kernel-Threads/"},{"name":"Threading Models","slug":"Threading-Models","link":"/tags/Threading-Models/"},{"name":"Maybe a rant","slug":"Maybe-a-rant","link":"/tags/Maybe-a-rant/"}],"categories":[{"name":"Architecture","slug":"Architecture","link":"/categories/Architecture/"},{"name":"Paradigms","slug":"Paradigms","link":"/categories/Paradigms/"},{"name":"Programming","slug":"Programming","link":"/categories/Programming/"}]}